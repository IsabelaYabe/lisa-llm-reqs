from dataclasses import dataclass, field
from typing import List, Optional

@dataclass
class PaperAnalysis:
    source_url: str
    datasets: Optional[List[str]] = field(default_factory=list)
    models: List[str] = field(default_factory=list)
    tasks: List[str] = field(default_factory=list)
    metrics: Optional[List[str]] = field(default_factory=list)
    
paper0 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1145/3652620.3686246",
    datasets=[],
    models=["GPT-4", "Llama", "Cohere"],
    tasks=["Automatically generate goal models from user stories", "Iterative Prompt Engineering"],
    metrics=[]
)
paper1 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/9848290",
    datasets=["SRS documents of 28 different systems"],
    models=["Multinomial Naive Bayes", "Gaussian Naive Bayes"],
    tasks=["Classify Actors and Use Cases from SRS documents", "Automate UML Use Case diagram generation"],
    metrics=["Accuracy"]
)
paper2 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10260954",
    datasets=["Software Engineering Contracts (SE Contracts)"],
    models=["GPT-3", "BART", "GPT-2", "T5", "Pegasus"],
    tasks=[
        "Generate SE-specific summaries from contract obligations",
        "Prompt Engineering for abstractive summarization",
        "Evaluate NLG models using ROUGE scores"
    ],
    metrics=["ROUGE"]
)
paper3 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10467677",
    datasets=["Diverse user stories corpora"],
    models=["N-gram model", "GPT-3"],
    tasks=[
        "Automatically generate user stories",
        "Compare user story quality using BLEU, ROUGE-N, and BERTScore metrics"
    ],
    metrics=["ROUGE-N", "BLEU", "BERTScore"]
)
paper4 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1007/s10515-022-00371-9",
    datasets=[],
    models=["QAExtractor", "QAPrioritiser (rule-based NLP framework)"],
    tasks=[
        "Extract quality attributes from user stories",
        "Prioritize quality attributes based on frequency, role impact, and criticality"
    ],
    metrics=["Precision", "Recall", "F-Measure"]
)
paper5 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1007/978-3-031-78386-9_20",
    datasets=[],
    models=["GPT-3.5", "GPT-4o"],
    tasks=["Generate user stories from project descriptions",
        "Assess quality of generated user stories",
        "Prioritize requirements "],
    metrics=["Semantic Similarity Score"]
)
paper6 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10893343",
    datasets=[],
    models=["GPT-4 Turbo"],
    tasks=[ "Generate user stories using LLMs and prompt engineering",
        "Compare effectiveness of user stories generated by traditional vs. AI-based methods",
        ],
    metrics=[]
)
paper7 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10544242",
    datasets=[],
    models=["gpt-3.5-turbo", "Llama-2-chat-hf"],
    tasks=["Categorize user stories to their sector domains",
        "Evaluate LLMs under zero-shot and few-shot prompting",
        "Automate user story categorization using prompt engineering"],
    metrics=["F-score"]
)
paper8 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10764984",
    datasets=["1,200 sub-features manually analyzed"],
    models=["LLM-based approach"],
    tasks=[
        "Compare feature elicitation approaches: App Store vs. LLM",
        "Analyze sub-features recommended by both methods",
        "Identify benefits and limitations of LLM-based inspiration"
        ],
    metrics=[]
)
paper9 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10663055",
    datasets=["Prompt examples mapped to SWEBoK knowledge areas"],
    models=["ChatGPT", "Mistral", "LLaMA"],
    tasks=[
        "Develop a catalog of prompts for software engineering training",
        "Evaluate prompt responses in tasks such as requirements elicitation, diagram generation, API simulation, effort estimation",
        "Assess LLMs in the context of software engineering pedagogy"
    ],
    metrics=["Qualitative analysis", "Quantitative analysis of quality, usefulness, and correctness"]
)
paper10 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1007/s42979-023-02212-2",
    datasets=["User stories (clustered using K-means and K-medoids)"],
    models=["K-means", "K-medoids"],
    tasks=[
        "Cluster user stories in Agile Software Development",
        "Compare clustering performance to increase cohesiveness among user stories"
    ],
    metrics=["Silhouette Coefficient"]
)
paper11 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10521022",
    datasets=["PROMISE_exp (1000 software requirements)", "PROMISE_IoT (2000 software and IoT requirements)"],
    models=["NLTK", "Chat-GPT"],
    tasks=[
        "Classify functional and non-functional requirements",
        "Classify requirements by system classes",
        "Generate unanticipated requirements for enhanced project success"
    ],
    metrics=[]
)
paper12 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1007/s10515-024-00448-7",
    datasets=[
        "Gold standard manually created by practitioners for evaluating user stories with acceptance criteria (US-AC)"
    ],
    models=["GPT-3", "GPT-3.5-turbo", "LLaMA series"],
    tasks=[
        "Simulate agile collaboration using role-based prompts with LLMs",
        "Generate acceptance criteria from user stories (GAC)",
        "Evaluate LLM-generated US-AC against human-written criteria",
        "Follow a create-update-update paradigm for AC generation"
    ],
    metrics=[
        "Completeness",
        "Validity",
        "INVEST"
    ]
)
paper13 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10256364",
    datasets=["40 selected papers from four computer science databases (systematic review corpus)"],
    models=[],
    tasks=[
        "Systematically review the use of user stories in requirements elicitation",
        "Identify common problems in user stories (e.g., ambiguity, incompleteness)",
        "Categorize research approaches addressing issues in user stories"
    ],
    metrics=[
        "Frequency count of problems in user stories"
    ]
)
paper14 = PaperAnalysis(
    source_url="https://dl.acm.org/doi/10.1145/3652620.3687810",
    datasets=[],
    models=["GPT-4", "Gemini"],
    tasks=[
        "Generate standard and UCM4IoT use case models from informal IoT problem descriptions"
    ],
    metrics=[
    ]
)
paper15 = PaperAnalysis(
    source_url="https://ieeexplore.ieee.org/document/10628461",
    datasets=[],
    models=["GPT-4", "CodeLlama"],
    tasks=["Generate Software Requirements Specification (SRS) documents from natural language",
        "Evaluate LLM-generated SRS against human-written benchmarks",
        "Identify and rectify issues in existing requirements documents",
        "Compare LLM outputs with entry-level software engineers"],
    metrics=["Eight quality criteria",
        "Human expert evaluation",
        "Time reduction analysis"]
)